{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Deep Learning\n",
        "\n",
        "# Tutorial 5: Softmax Regression for classification\n",
        "\n",
        "In this tutorial, we will cover:\n",
        "\n",
        "- Softmax Regression\n",
        "\n",
        "Prerequisites:\n",
        "\n",
        "- Python\n",
        "\n",
        "My contact:\n",
        "\n",
        "- Niklas Beuter (niklas.beuter@th-luebeck.de)\n",
        "\n",
        "Course:\n",
        "\n",
        "- Slides and notebooks will be available at https://lernraum.th-luebeck.de/course/view.php?id=5383"
      ],
      "metadata": {
        "id": "_44n665wAWxv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Softmax Regression\n",
        "\n",
        "Softmax regression, also known as multinomial logistic regression, is a generalization of logistic regression that can handle multiple classes, as opposed to binary classification which deals with only two classes. It is widely used for classification tasks where the classes are mutually exclusive. For example, it is used for handwriting recognition where each image only corresponds to one digit.\n",
        "\n",
        "### The Softmax Function\n",
        "\n",
        "The core of Softmax regression is the Softmax function. This function takes a vector of raw scores, often called logits, and transforms it into a vector of probabilities, with each value representing the probability that the input belongs to a corresponding class. The probabilities must be positive and sum up to one so that they can be directly interpreted as probabilities.\n",
        "\n",
        "The Softmax function is defined as follows for an input vector **z**:\n",
        "\n",
        "$$ Softmax(z)_i = \\frac{e^{z_i}}{\\sum_{j=1}^{K} e^{z_j}} $$\n",
        "\n",
        "Here, $ e^{z_i} $ represents the exponential of the ith element of the input vector **z**, and $ K $ is the number of classes.\n",
        "\n",
        "### Learning the Parameters\n",
        "\n",
        "Similar to logistic regression, Softmax regression models learn weights and biases through the training process. However, instead of fitting a single line (or hyperplane in higher dimensions), Softmax regression fits one line for each class. The decision boundary between any two classes is then determined by the points where their probabilities are equal.\n",
        "\n",
        "The parameters are typically learned by minimizing the cross-entropy loss, also known as the negative log-likelihood. This loss function is a measure of how well the predicted probability distribution fits the true distribution (the true class labels). For a single data point **x**, the cross-entropy loss for true class **k** is:\n",
        "\n",
        "$$ L(\\theta) = -\\sum_{i=1}^{K} y_i \\log(p_i) $$\n",
        "\n",
        "where $ y_i $ is the binary indicator (0 or 1) if class label **c** is the correct classification for **x**, and $ p_i $ is the predicted probability that **x** is of class **i**.\n",
        "\n",
        "### Optimization\n",
        "\n",
        "To find the best parameters, we use optimization algorithms such as gradient descent or its variants (stochastic gradient descent, mini-batch gradient descent) to minimize the cross-entropy loss over the entire training set. The optimization is done iteratively by computing the gradient of the loss with respect to each parameter and adjusting the parameters in the direction that reduces the loss.\n",
        "\n",
        "### Advantages of Softmax Regression\n",
        "\n",
        "- **Flexibility**: It can handle multiple classes directly, without the need to reduce the problem into multiple binary classification problems.\n",
        "- **Probabilistic Interpretation**: The output can be interpreted as a probability distribution over classes.\n",
        "- **Efficiency**: It is computationally efficient, especially with the use of vectorized operations.\n",
        "\n",
        "Softmax regression is a powerful tool for classification problems and serves as a foundational model for more complex neural network architectures used in deep learning."
      ],
      "metadata": {
        "id": "wO46yRNUBeJp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Going into the code\n",
        "\n",
        "First, we import all needed modules"
      ],
      "metadata": {
        "id": "ugujnR2lC-cm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2CXIiD1qAGmk"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from sklearn.datasets import make_blobs\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as mpatches"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then, we create a dataset to work on."
      ],
      "metadata": {
        "id": "LnpxkTM6DFQV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate a synthetic dataset\n",
        "X, y = make_blobs(n_samples=100, centers=3, n_features=2, random_state=42)\n",
        "\n",
        "# Convert the dataset to PyTorch tensors\n",
        "X_tensor = torch.tensor(X, dtype=torch.float32)\n",
        "y_tensor = torch.tensor(y, dtype=torch.long)"
      ],
      "metadata": {
        "id": "FyvVRN01AZiH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's visualize the data to understand with what we are working"
      ],
      "metadata": {
        "id": "cYtD8dwMDIb9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize the dataset\n",
        "plt.scatter(X[:, 0], X[:, 1], c=y, cmap='viridis')\n",
        "plt.xlabel('Feature 1')\n",
        "plt.ylabel('Feature 2')\n",
        "plt.title('Synthetic Dataset for Softmax Regression')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "geZJRfoIAeLv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let us define the standard linear model and adding the softmax (directly from torch).\n",
        "We also define the cross entropy loss."
      ],
      "metadata": {
        "id": "OoN5qPXpDL5d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def model(x):\n",
        "    return torch.softmax(x @ weights + bias, dim=1)\n",
        "\n",
        "def cross_entropy_loss(predictions, targets):\n",
        "    log_p = -torch.log(predictions[range(targets.shape[0]), targets])\n",
        "    return torch.mean(log_p)"
      ],
      "metadata": {
        "id": "_e2vCes7Af7w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As model parameters we only have 2 features (x and y) and 3 classes. So, we define three output linear functions.\n",
        "\n",
        "Let us try to predict first input values to see, if everything works."
      ],
      "metadata": {
        "id": "U9FHq1jSDUr1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Model parameters\n",
        "weights = torch.randn((2, 3), requires_grad=True)  # 2 features, 3 classes\n",
        "bias = torch.zeros(3, requires_grad=True)\n",
        "\n",
        "# Example prediction\n",
        "preds = model(X_tensor)\n",
        "print(preds[:5])  # Display the first 5 predictions"
      ],
      "metadata": {
        "id": "pkAHNPkhAl6Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we should start to train our softmax regression network. We set the learning rate and the number of epochs for training."
      ],
      "metadata": {
        "id": "1huyvHhnD3X9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Learning rate\n",
        "lr = 0.1\n",
        "# Number of epochs\n",
        "epochs = 100"
      ],
      "metadata": {
        "id": "CyYbCPWSAsAo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let us start training and optimize the weights and biases."
      ],
      "metadata": {
        "id": "yDd9qrt0D-tc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Training loop\n",
        "for epoch in range(epochs):\n",
        "    preds = model(X_tensor)\n",
        "    loss = cross_entropy_loss(preds, y_tensor)\n",
        "    loss.backward()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        weights -= lr * weights.grad\n",
        "        bias -= lr * bias.grad\n",
        "        weights.grad.zero_()\n",
        "        bias.grad.zero_()\n",
        "\n",
        "    if epoch % 10 == 0:\n",
        "        print(f'Epoch {epoch}, Loss: {loss.item()}')"
      ],
      "metadata": {
        "id": "E1fuUtu1AsjI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we can visualize the result and predict a new data point."
      ],
      "metadata": {
        "id": "XctH4hqTECZ1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a function to predict the class and visualize it\n",
        "def predict_and_visualize(new_point):\n",
        "    # Convert the new point to a PyTorch tensor\n",
        "    new_point_tensor = torch.tensor(new_point, dtype=torch.float32).unsqueeze(0)\n",
        "\n",
        "    # Use the trained model to predict the class\n",
        "    prediction = model(new_point_tensor)\n",
        "    predicted_class = torch.argmax(prediction).item()\n",
        "\n",
        "    # Now let's plot the original dataset\n",
        "    plt.scatter(X[:, 0], X[:, 1], c=y, cmap='viridis')\n",
        "\n",
        "    # And plot the new point with the predicted class\n",
        "    color_map = plt.cm.viridis\n",
        "    class_colors = [color_map(i) for i in [0.0, 0.5, 1.0]]\n",
        "    predicted_color = class_colors[predicted_class]\n",
        "\n",
        "    plt.scatter(new_point[0], new_point[1], color=predicted_color, edgecolor='black', s=100, zorder=3)\n",
        "    plt.xlabel('Feature 1')\n",
        "    plt.ylabel('Feature 2')\n",
        "    plt.title('Synthetic Dataset with Predicted Point')\n",
        "\n",
        "    # Create a legend for the new point\n",
        "    legend_patch = mpatches.Patch(color=predicted_color, label=f'Predicted class: {predicted_class}')\n",
        "    plt.legend(handles=[legend_patch])\n",
        "\n",
        "    plt.show()\n",
        "    return predicted_class"
      ],
      "metadata": {
        "id": "PfKd6C1NA2S5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, you can set any point you like and see the classification result."
      ],
      "metadata": {
        "id": "KyBpt7A6EGsV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage: Let's predict the class of a new point (1.0, -10.0)\n",
        "new_data_point = [0.0, -2.0]\n",
        "predicted_class = predict_and_visualize(new_data_point)\n",
        "print(f\"The predicted class for new data point {new_data_point} is {predicted_class}\")"
      ],
      "metadata": {
        "id": "6SNJM4RpA4IC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Plot decision boundaries\n",
        "\n",
        "Finally, let us see the result of the model. So, we ploit the decision boundaries."
      ],
      "metadata": {
        "id": "7RcDHRKvEKqD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_linear_discriminants(weights, bias):\n",
        "    # Generate a grid of points to plot decision boundaries\n",
        "    x_min, x_max = X_tensor[:, 0].min() - 1, X_tensor[:, 0].max() + 1\n",
        "    y_min, y_max = X_tensor[:, 1].min() - 1, X_tensor[:, 1].max() + 1\n",
        "    h = 0.01\n",
        "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
        "\n",
        "    # Flatten the grid to pass through the model\n",
        "    grid = np.c_[xx.ravel(), yy.ravel()]\n",
        "    grid_tensor = torch.tensor(grid, dtype=torch.float32)\n",
        "\n",
        "    # Compute the class scores from the linear part (before softmax)\n",
        "    scores = grid_tensor @ weights + bias.unsqueeze(0)\n",
        "\n",
        "    # Compute the predicted class labels\n",
        "    _, predicted_classes = torch.max(scores, 1)\n",
        "    predicted_classes = predicted_classes.reshape(xx.shape)\n",
        "\n",
        "    # Plot the decision boundary\n",
        "    plt.contourf(xx, yy, predicted_classes.numpy(), alpha=0.5, cmap='viridis')\n",
        "\n",
        "    # Plot the original data points for reference\n",
        "    plt.scatter(X[:, 0], X[:, 1], c=y, edgecolor='k', cmap='viridis')\n",
        "    plt.xlabel('Feature 1')\n",
        "    plt.ylabel('Feature 2')\n",
        "    plt.title('Linear Discriminant Functions')\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "BddxV8iwA6eP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot linear discriminants after training\n",
        "plot_linear_discriminants(weights, bias)"
      ],
      "metadata": {
        "id": "MM7lFomiA8PK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}